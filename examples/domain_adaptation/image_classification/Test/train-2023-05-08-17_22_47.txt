Namespace(arch='resnet50', batch_size=16, bottleneck_dim=256, data='Both', epochs=150, iters_per_epoch=500, kich_ban='S2T', label=3, log='Test/', loss_function='MKME', lr=0.003, lr_decay=0.75, lr_gamma=0.0003, momentum=0.9, no_pool=False, non_linear=False, per_class_eval=True, phase='train', print_freq=100, random_frequencies=5, scale_parameter=1, scratch=False, seed=None, trade_off=1.0, wd=0.0005, workers=2)
Concate data
=> using model 'resnet50'
Epoch: [0][  0/500]	Time 4.13 (4.13)	Data 0.0 (0.0)	Loss 1.06 (1.06)	Trans Loss 0.0000 (0.0000)	Cls Acc 31.2 (31.2)
Epoch: [0][100/500]	Time 0.10 (0.14)	Data 0.0 (0.0)	Loss 0.40 (0.74)	Trans Loss 0.0000 (0.0000)	Cls Acc 81.2 (63.5)
Traceback (most recent call last):
  File "custom_dan_EU.py", line 627, in <module>
    main(args)
  File "custom_dan_EU.py", line 395, in main
    lr_scheduler, epoch, args)
  File "custom_dan_EU.py", line 530, in train
    optimizer.step()
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/optim/optimizer.py", line 23, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/optim/sgd.py", line 161, in step
    foreach=group['foreach'])
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/optim/sgd.py", line 211, in sgd
    maximize=maximize)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/optim/sgd.py", line 238, in _single_tensor_sgd
    buf.mul_(momentum).add_(d_p, alpha=1 - dampening)
KeyboardInterrupt
