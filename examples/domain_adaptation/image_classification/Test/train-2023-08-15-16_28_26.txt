Namespace(arch='convnext_xlarge_384_in22ft1k', batch_size=8, bottleneck_dim=256, byte_size=256, data='Both', epochs=2, iters_per_epoch=2, label=3, log='Test/', loss_function='MKMMD', lr=0.003, lr_decay=0.75, lr_gamma=0.0003, momentum=0.9, no_pool=False, non_linear=False, per_class_eval=True, phase='train', print_freq=100, random_frequencies=5, scale_parameter=1, scenario='S2T', scratch=False, seed=None, test_statistic='none', trade_off=1.0, wd=0.0005, workers=2)
Concate data
num_classes: 3
=> using model 'convnext_xlarge_384_in22ft1k'
Downloading: "https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_1k_384_ema.pth" to /home/bkcs/.cache/torch/hub/checkpoints/convnext_xlarge_22k_1k_384_ema.pth
Traceback (most recent call last):
  File "custom_dan.py", line 850, in <module>
    main(args)
  File "custom_dan.py", line 585, in main
    lr_scheduler, epoch, args)
  File "custom_dan.py", line 713, in train
    y_t, f_t = model(x_t)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/tllib/modules/classifier.py", line 80, in forward
    f = self.pool_layer(self.backbone(x))
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/timm/models/convnext.py", line 411, in forward
    x = self.forward_features(x)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/timm/models/convnext.py", line 398, in forward_features
    x = self.stages(x)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/timm/models/convnext.py", line 250, in forward
    x = self.blocks(x)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/container.py", line 204, in forward
    input = module(input)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/timm/models/convnext.py", line 183, in forward
    x = self.norm(x)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/timm/models/layers/norm.py", line 55, in forward
    x = F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/functional.py", line 2515, in layer_norm
    return torch.layer_norm(input, normalized_shape, weight, bias, eps, torch.backends.cudnn.enabled)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 9.77 GiB total capacity; 7.83 GiB already allocated; 16.00 MiB free; 8.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
