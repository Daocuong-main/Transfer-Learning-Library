Namespace(arch='resnet18', batch_size=16, bottleneck_dim=256, byte_size=256, data='nondan', epochs=250, iters_per_epoch=500, label=3, log='Test/Change_backbone/S2T/', loss_function='None', lr=0.003, lr_decay=0.75, lr_gamma=0.0003, momentum=0.9, no_pool=False, non_linear=False, per_class_eval=True, phase='train', print_freq=100, random_frequencies=5, scale_parameter=1, scenario='S2T', scratch=False, seed=None, test_statistic='none', trade_off=0.0, wd=0.0005, workers=2)
nondan
num_classes: 5
=> using model 'resnet18'
Downloading: "https://download.pytorch.org/models/resnet18-f37072fd.pth" to /home/bkcs/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
  0%|          | 0.00/44.7M [00:00<?, ?B/s]  8%|8         | 3.71M/44.7M [00:00<00:01, 38.9MB/s] 18%|#7        | 7.88M/44.7M [00:00<00:00, 39.5MB/s] 26%|##6       | 11.7M/44.7M [00:00<00:00, 39.9MB/s] 35%|###4      | 15.5M/44.7M [00:00<00:01, 19.2MB/s] 42%|####2     | 18.8M/44.7M [00:00<00:01, 22.0MB/s] 48%|####8     | 21.6M/44.7M [00:00<00:01, 19.9MB/s] 54%|#####3    | 24.0M/44.7M [00:01<00:01, 17.4MB/s] 58%|#####8    | 26.0M/44.7M [00:01<00:01, 16.9MB/s] 62%|######2   | 27.9M/44.7M [00:01<00:01, 16.6MB/s] 66%|######6   | 29.6M/44.7M [00:01<00:00, 15.9MB/s] 70%|######9   | 31.2M/44.7M [00:01<00:00, 15.5MB/s] 73%|#######3  | 32.7M/44.7M [00:01<00:00, 15.4MB/s] 77%|#######6  | 34.2M/44.7M [00:01<00:00, 14.7MB/s] 80%|#######9  | 35.7M/44.7M [00:02<00:00, 14.7MB/s] 83%|########3 | 37.2M/44.7M [00:02<00:00, 14.9MB/s] 86%|########6 | 38.6M/44.7M [00:02<00:00, 14.9MB/s] 90%|########9 | 40.0M/44.7M [00:02<00:00, 14.7MB/s] 93%|#########2| 41.4M/44.7M [00:02<00:00, 14.7MB/s] 96%|#########5| 42.9M/44.7M [00:02<00:00, 14.7MB/s] 99%|#########9| 44.3M/44.7M [00:02<00:00, 14.5MB/s]100%|##########| 44.7M/44.7M [00:02<00:00, 17.6MB/s]
Traceback (most recent call last):
  File "custom_dan.py", line 850, in <module>
    main(args)
  File "custom_dan.py", line 487, in main
    pool_layer=pool_layer, finetune=not args.scratch).to(device)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 989, in to
    return self._apply(convert)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 641, in _apply
    module._apply(fn)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 664, in _apply
    param_applied = fn(param)
  File "/home/bkcs/miniconda3/lib/python3.7/site-packages/torch/nn/modules/module.py", line 987, in convert
    return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None, non_blocking)
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
